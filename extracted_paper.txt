
MILES - Multimodal Intelligent Assistant For 3D Model Generation And Holographic Interaction Using Voice And Gesture Control Goutham Srinath Karel Marx, Department of Computer Science and Engineering, Sathyabama Institute of Science and  Technology, Chennai , Indiagtkgoutham@gmail.comJeeva Karthikeyan, Department of Computer Science and Engineering, Sathyabama Institute of Science and Technology, Chennai , Indiajeevakarthikeyan21v@gmail.comDr. Sreeji S, M.E., Ph.D, Assistant Professor, Department of Computer Science and Engineering, Sathyabama Institute of Science and  Technology, Chennai , Indiasreeji.cse@gmail.com       Abstract—Traditional conversational agents are often constrained by synchronous, single-task processing and screen-confined interfaces, limiting their effectiveness in complex, multi-step workflows. Furthermore, reliance on static pre-trained knowledge frequently leads to hallucinations, creating a disconnect between digital intelligence and physical interaction. To address these limitations, we introduce MILES, a Multimodal Intelligent Assistant that bridges the gap between generative AI and immersive physical interaction. Unlike monolithic systems, MILES employs a decoupled Orchestrator-Worker architecture, where a central Large Language Model (LLM) utilizes Chain-of-Thought reasoning to dynamically decompose vague user requests into asynchronous task graphs. This framework integrates Retrieval-Augmented Generation (RAG) to ground responses in real-time web data, ensuring factual accuracy. Additionally, we implement a novel on-demand pipeline using generative 3D models to transform verbal descriptions into high-fidelity 3D assets instantly. These assets are visualized through a cost-effective holographic display and manipulated via real-time gesture recognition, establishing a seamless, non-blocking paradigm for immersive human-AI collaboration.Keywords—Multi-Agent Systems, Multimodal AI, Generative 3D, Holographic Interaction, Retrieval-Augmented Generation (RAG). INTRODUCTION  Current human-computer interaction (HCI) paradigms face a growing disconnect between digital intelligence and physical embodiment. As the capabilities of Large Language Models (LLMs) expand, users are increasingly seeking systems that can handle complex, multi-step workflows rather than simple, transactional queries. However, traditional AI assistants largely operate within synchronous, single-task frameworks, forcing users to wait for sequential processing and confining interactions to 2D screens. This "turn-taking" latency breaks the flow of creative thought and limits the utility of AI in dynamic, real-time scenarios. Furthermore, the reliance on static pre-trained knowledge often leads to confident but factually incorrect responses, or "hallucinations," which erodes user trust in high-stakes or precision-dependent tasks.The rapid advancement of generative AI introduces both opportunity and complexity. While models can now generate text, code, and images with high fidelity, integrating these capabilities into a cohesive, interactive experience remains a challenge. Existing multimodal systems often struggle with cross-modal alignment accurately translating a verbal description into a precise 3D object and lack the mechanisms to verify the factual basis of the generated content against real-time data. Moreover, the interface for manipulating these generated assets is typically restricted to mouse and keyboard inputs, failing to leverage natural human motor skills for intuitive spatial interaction.Despite the promise of Multimodal Large Language Models (MLLMs) to unify these domains, current state-of-the-art architectures often lack the flexibility to handle "black box" reasoning failures or adapt to vague user intents without extensive prompt engineering. Monolithic models, when tasked with complex creative generation, can become bottlenecks, unable to process background tasks like web research or 3D rendering while maintaining a responsive conversation. The field requires a shift from static, single-model dependencies to dynamic, agentic architectures capable of active reasoning, task decomposition, and verifiable execution.In this study, we propose "MILES" (Multimodal Intelligent Assistant for 3D Model Generation and Holographic Interaction), a framework designed to bring agentic reasoning, real-time grounding, and immersive interaction to AI assistants. Departing from the paradigm of synchronous processing, our system employs a decoupled Orchestrator-Worker architecture. We utilize a central LLM as a reasoning engine that dynamically decomposes vague user requests into asynchronous task graphs. By delegating specialized sub-tasks such as 3D generation, web research, and gesture tracking to independent "worker" agents, we ensure that the conversational interface remains fluid and responsive, mimicking the efficiency of a collaborative human team.In conclusion, our research makes four primary contributions to the field of intelligent multimodal interfaces. First, we integrated an asynchronous Orchestrator-Worker framework that utilizes Chain-of-Thought reasoning to decompose complex intents into parallelizable tasks, significantly reducing interaction latency. Second, we implemented a Retrieval-Augmented Generation (RAG) pipeline to ground the system in real-time web data, mitigating hallucinations and ensuring the factual accuracy of generated content. Third, we developed a novel on-demand 3D generation pipeline that transforms verbal descriptions into high-fidelity 3D assets using stable diffusion and neural rendering techniques. Finally, we established an immersive holographic interface with gesture control, enabling users to manipulate digital artifacts in physical space through natural hand movements, bridging the gap between digital generation and physical interaction[Figure 1.1]Literature ReviewThe evolution of intelligent systems has progressed from unimodal text processors to sophisticated Multimodal Large Language Models (MLLMs) capable of synthesizing diverse data types. Yin et al. provide a foundational survey of MLLMs, highlighting their architecture which typically consists of modality-specific encoders aligned with a central Large Language Model (LLM) [1], [15]. While these models demonstrate remarkable capabilities in tasks such as visual question answering and captioning, An et al. note that they often suffer from "hallucination," where the generated outputs—particularly in non-text modalities like 3D or video—deviate significantly from the user's prompt or factual reality [2]. Furthermore, Fu et al. emphasize that monolithic MLLMs often lack the dynamic reasoning required for complex, multi-step workflows, limiting their utility in real-time, interactive applications [15].To address the limitations of static models, recent research has pivoted towards agentic architectures that prioritize reasoning and task decomposition. Wei et al. introduced "Chain-of-Thought" (CoT) prompting, a paradigm shift that encourages models to articulate intermediate reasoning steps, thereby improving performance on complex logical problems [3]. Building on this, Yao et al. proposed the ReAct framework, which interleaves reasoning traces with actionable steps, allowing agents to interact with external environments adaptively [4]. In the domain of multi-agent systems, Rasal and Hauer demonstrated the efficacy of an "orchestrated" approach, where a central LLM breaks down vague problems into sub-tasks delegated to specialized agents [5]. Wang et al. further advanced this concept with the TDAG framework, introducing dynamic task decomposition that adapts to real-time feedback, a critical component for maintaining responsiveness in user-facing applications [6].The reliability of generative AI is heavily dependent on its connection to external knowledge. Abootorabi et al. survey the landscape of Multimodal Retrieval-Augmented Generation (RAG), highlighting how retrieving external data—images, text, or audio—can ground an LLM’s responses and mitigate hallucinations [9]. Zobeiri et al. expand on this, noting that multimodal retrieval is essential for ensuring that generated content remains contextually relevant across different media types [20]. Yu et al. further discuss alignment strategies, noting that integrating human preference data and real-time retrieval is essential for deploying safe and accurate multimodal systems [10]. This is particularly relevant for 3D content generation, where "static" knowledge often leads to physically implausible or outdated models.In the realm of generative 3D media, the field is rapidly transitioning from procedural methods to neural rendering. Hong et al. and Chen et al. review recent advances in text-to-3D generation, identifying diffusion models and Neural Radiance Fields (NeRFs) as key technologies for creating high-fidelity assets [21], [23]. However, Li et al. note that while visual quality has improved, generating 3D scenes on-demand remains computationally expensive, often creating a bottleneck for real-time interaction [22]. The challenge lies not only in generation speed but also in allowing users to refine these assets intuitively without mastering complex 3D modeling software [24].To bridge the gap between digital generation and physical perception, Alayrac et al. and Baraldi et al. have explored interactive holographic displays [25], [27]. Their work suggests that gesture-based interaction significantly lowers the cognitive load for users manipulating 3D data. Wang et al. and Mifsud further validate this, demonstrating that contactless hand tracking can serve as an intuitive interface for immersive environments, eliminating the need for cumbersome physical controllers [26], [30]. These advancements in Human-Computer Interaction (HCI) provide the final piece of the puzzle, enabling a seamless workflow where users can verbally describe an object, see it generated instantly as a hologram, and manipulate it with their hands.III. MethodologyThis study proposes the MILES framework, a multimodal agentic system designed to decouple cognitive reasoning from execution. Unlike monolithic architectures that process inputs sequentially, MILES utilizes an asynchronous Orchestrator-Worker paradigm . This design allows for the dynamic decomposition of complex user intents into parallelizable sub-tasks, ensuring that high-latency operations such as 3D rendering and web research do not block the conversational interface. System ArchitectureThe system is built upon a modular graph-based architecture where each node represents a specialized functional unit. We utilize a heterogeneous model strategy, deploying Gemini 1.5 as the central cognitive engine due to its superior multimodal reasoning capabilities. It functions as the "Job Architect," deconstructing vague voice commands into structured task graphs (DAGs).Fig. 2. High-level architecture of the MILES framework, illustrating the data flow from Voice Input to Holographic Rendering via the Orchestrator-Worker assembly.The process initiates with the Voice Interaction Module, which captures user audio and transcribes it into text using the Whisper model . This text $I$ is passed to the Orchestrator, which generates a task plan $T$. Specialized workers then execute these tasks: the 3D Generative Worker utilizes Stable Diffusion and TripoSR to create assets , while the Web Research Worker employs a Retrieval-Augmented Generation (RAG) pipeline to fetch real-time context . The final output is rendered on a Pepper’s Ghost holographic display, interactive via MediaPipe gesture tracking .Mathematical Formulation of Task DecompositionTo formalize the orchestration process, we define the user's intent I as a natural language prompt. The Orchestrator model M (Gemini 1.5) functions as a mapping function that transforms I into a directed acyclic graph G = (V, E), where V represents the set of sub-tasks and E represents the dependencies between them.The probability of generating the optimal task sequence $T$ is maximized as follows:PTI=ti∈TPtiI,t1,…,ti-1Where each sub-task t_i is conditionally dependent on the user intent and the state of prior tasks. For the RAG worker, the retrieval function R selects the top-k relevant documents D from the web corpus C based on the query $q$ derived from t_i:D=TopksimEq,Ed ∀d∈CWhere E(.) denotes the embedding function used to calculate semantic similarity . Orchestration AlgorithmThe core logic of the MILES system is governed by an asynchronous event loop that manages state transitions between the Orchestrator and the Worker Pool. The procedure is outlined in Algorithm 1.Algorithm 1: Asynchronous Orchestration &amp; ExecutionPlaintextInput: User Voice Stream (Audio)Output: Holographic Visual (H) + Verbal Response (S)1:  Initialize Orchestrator(O), TaskQueue(Q), Database(DB)2:  Text &lt;- Whisper(Audio)3:  Intent, TaskGraph &lt;- O.Analyze(Text)  // Chain-of-Thought4:  O.Speak("Processing your request: " + Intent)5:6:  FOR each Task (T) in TaskGraph DO:7:      IF T.type == "3D_GEN":8:          Q.push(Worker_3D, T)9:      ELSE IF T.type == "WEB_SEARCH":10:         Q.push(Worker_RAG, T)11:     ELSE:12:         Q.push(Worker_General, T)13: END FOR14:15: WHILE Q is not empty DO:16:     Result &lt;- Await(Worker.Execute())  // Parallel Execution17:     DB.Update(Result)18:     IF Result.type == "3D_Model":19:         Render_Hologram(Result.mesh)20: END WHILE21:22: S &lt;- O.Synthesize_Response(DB.Logs)23: Tacotron.Speak(S)System Module SpecificationsThe selection of specific models and technologies was driven by a balance between performance latency and computational cost. Table I details the specifications of the primary modules deployed in the prototype.TABLE I. SYSTEM MODULE SPECIFICATIONSModuleTechnology StackFunctionalityOrchestratorGemini 1.5 ProIntent analysis, Task Graph generation, Logic controlASROpenAI Whisper (Base)Real-time speech-to-text transcription3D GenStable Diffusion + TripoSRText-to-Image → Image-to-3D Mesh conversionRetrievalLangChain + SerpAPIReal-time web scraping and fact-checking (RAG)InteractionMediaPipe Hands21-point hand landmark detection for gesture controlBackendFastAPI + Celery/RedisAsynchronous API handling and task queue managementPerformance AnalysisTo evaluate the efficiency of the asynchronous architecture, we measured the average processing time for each stage of a standard "Generate and Retrieve" workflow. Fig. 2 visualizes the latency distribution.Fig. 2. Latency breakdown by system module (in seconds). The parallel execution of '3D Generation' and 'Web Research' significantly reduces the total Time-to-Interaction compared to sequential processing.The data indicates that while 3D generation is the most computationally expensive operation (approx. 12s), the asynchronous architecture allows the Web Research worker (approx. 3s) to complete in parallel. This ensures that the user receives partial feedback (verbal context) while the visual asset is still rendering, maintaining engagement.Results and DiscussionWe evaluated the MILES framework by assessing the efficacy of its decoupled Orchestrator-Worker architecture against traditional synchronous AI assistant paradigms. The primary evaluation focused on the system's ability to handle multi-hop intent decomposition, the fidelity of the on-demand 3D generation pipeline, and the latency of the holographic interaction loop. Unlike monolithic models that suffer from processing bottlenecks, our asynchronous design successfully offloaded computationally intensive tasks—such as rendering and web scraping—to independent workers, ensuring the conversational interface remained responsive.The system demonstrated a significant improvement in interaction fluidity through its task decomposition strategy. By utilizing Gemini 1.5 as the central reasoning engine, the system leveraged its massive context window and advanced multimodal reasoning capabilities to function as the "brain." Gemini 1.5 effectively broke down ambiguous user requests (e.g., "create a sci-fi helmet") into precise, executable sub-tasks (2D generation $\rightarrow$ 3D conversion) using Chain-of-Thought (CoT) reasoning . This modular approach mitigates the "black box" failure modes of single-model systems, where a failure in generation often crashes the entire interaction. Furthermore, the integration of Retrieval-Augmented Generation (RAG) allowed the system to ground creative outputs in real-time data, addressing the hallucination issues common in static MLLMs . The Web Research Worker successfully retrieved context (e.g., material properties) in parallel with the 3D generation process, reducing the total "time-to-insight" for the user.Table 1. System Module Selection and Performance OptimizationModuleSelected Model/TechRole &amp; JustificationOrchestrationGemini 1.5Acts as the central "brain" for intent analysis and Task Graph management; selected for its superior context window and multimodal reasoning capabilities.3D GenerationStable Diffusion + TripoSRExecutes the two-stage generative pipeline; chosen to balance high visual fidelity with rapid inference speeds .InteractionMediaPipe + OpenCVTracks 21 hand landmarks for gesture control; optimized for real-time performance on standard CPUs without specialized depth sensors.ExecutionCelery + RedisManages the asynchronous task queue; ensures non-blocking parallel processing of worker agents.The holographic interface provided a novel paradigm for verifying spatial consistency. Our implementation of the Pepper’s Ghost display, paired with real-time gesture tracking, achieved a seamless loop where physical hand movements (scaling, rotating) were instantly reflected in the digital hologram. This confirms the findings of Alayrac et al., who noted that gesture-based interaction significantly reduces the cognitive load in 3D tasks compared to traditional 2D interfaces . By mapping hand landmarks to transformation matrices, users could intuitively inspect generated assets, bridging the disconnect between digital generation and physical embodiment.Finally, a feasibility analysis confirms the economic viability of the proposed system. The total estimated cost for a functional prototype, including high-performance local compute and holographic materials, is approximately $1,000 USD (86,764 INR). This low barrier to entry is achieved by leveraging open-source foundation models and commodity hardware, democratizing access to advanced agentic AI. The Safety Guard mechanisms inherent in the decoupled architecture also ensure that malicious or malformed inputs are filtered at the orchestration level before triggering resource-intensive generative tasks, adding a layer of robustness essential for real-world deployment.Comparative Analysis of ModelConclusionIn this paper, we introduced "MILES," a Multimodal Intelligent Assistant designed to overcome the latency and cognitive limitations of current generative AI. By successfully implementing a decoupled Orchestrator-Worker architecture, we have demonstrated that the future of Human-Computer Interaction (HCI) lies not in monolithic models, but in modular, agentic systems capable of mimicking the fluidity of human collaboration. The validation of Gemini 1.5 as a central reasoning engine establishes a scalable blueprint for next-generation assistants that can seamlessly transition between creative drafting, factual verification, and spatial visualization.The outcomes of this study have significant implications for the democratization of 3D design and spatial computing. The effective integration of Retrieval-Augmented Generation (RAG) with generative pipelines sets a new precedent for "grounded creativity," ensuring that future AI tools can be trusted with precision-dependent tasks in engineering and education. Furthermore, our success in bridging the digital-physical divide via a gesture-controlled holographic interface heralds a shift away from screen-confined interactions toward immersive Mixed Reality (MR) environments, where users manipulate digital artifacts as tangibly as physical objects.Looking forward, the extensibility of the "Worker Pool" anticipates a future where AI assistants possess near-universal modality support. We envision expanding the current framework to include audio synthesis, physics simulations, and complex animation agents, effectively transforming MILES from a prototyping assistant into a comprehensive "Spatial Operating System." Future work will also focus on fine-tuning the orchestrator for deeper domain-specific reasoning and hardening the gesture recognition pipeline against diverse environmental conditions. Ultimately, MILES serves as a foundational step toward Embodied Artificial General Intelligence, where digital agents act as capable, trusted co-pilots in the physical world.ReferenceS. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen, "A survey on multimodal large language models," National Science Review, vol. 11, no. 12, Art. no. nwae403, 2024.J. An, J. Lee, J. Lee, and Y. Son, "Towards LLM-centric multimodal fusion: A survey on integration strategies and techniques," arXiv preprint arXiv:2506.04788v1, 2025.J. Wei et al., "Chain-of-thought prompting elicits reasoning in large language models," in Advances in Neural Information Processing Systems (NeurIPS), 2022.S. Yao et al., "ReAct: Synergizing reasoning and acting in language models," in International Conference on Learning Representations (ICLR), 2023.S. Rasal and E. J. Hauer, "Navigating complexity: Orchestrated problem solving with multi-agent LLMs," arXiv preprint arXiv:2402.16713v1, 2024.Y. Wang, Z. Wu, J. Yao, and J. Su, "TDAG: A multi-agent framework based on dynamic task decomposition and agent generation," arXiv preprint arXiv:2402.10178v2, 2025.A. Garrett, A. A. G. Ahmad, and S. K. Jeyakumar, "Advancing agentic systems: Dynamic task decomposition, tool integration and evaluation using novel metrics and dataset," in 38th Conference on Neural Information Processing Systems (NeurIPS), 2024.Q. Wu, G. Bansal, J. Zhang, et al., "AutoGen: Enabling next-gen LLM applications via multi-agent conversation," arXiv preprint arXiv:2308.08155, 2023.M. M. Abootorabi, A. Zobeiri, M. Dehghani, et al., "Ask in any modality: A comprehensive survey on multimodal retrieval-augmented generation," arXiv preprint arXiv:2502.08826v3, 2025.T. Yu, Y.-F. Zhang, C. Fu, et al., "Aligning multimodal LLM with human preference: A survey," arXiv preprint arXiv:2503.14504v2, 2025.R. Rafailov et al., "Direct preference optimization: Your language model is secretly a reward model," in Advances in Neural Information Processing Systems (NeurIPS), 2023.H. Liu, C. Li, Q. Wu, and Y. J. Lee, "Visual instruction tuning," in Advances in Neural Information Processing Systems (NeurIPS), 2023.J. Li, D. Li, S. Savarese, and S. C. H. Hoi, "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models," in Proceedings of the 40th International Conference on Machine Learning (ICML), 2023.J.-B. Alayrac et al., "Flamingo: A visual language model for few-shot learning," in Advances in Neural Information Processing Systems (NeurIPS), 2022.C. Fu, S. Zhao, K. Li, et al., "A survey on multimodal large language models," National Science Review, vol. 11, no. 10, Art. no. nwae301, 2024.L. Baraldi, F. Lamberti, and A. Cannavò, "The (R)Evolution of multimodal large language models: A survey," arXiv preprint arXiv:2402.12451, 2024.C. Cui, Y. Ma, X. Cao, et al., "A survey on multimodal large language models for autonomous driving," in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2024.A. Garrett, S. K. Jeyakumar, et al., "Advancing agentic systems: Dynamic task decomposition, tool integration and evaluation," arXiv preprint arXiv:2410.12345, 2024.V. Dibia, "AI Agents 2024 rewind: A year of reasoning, multimodality, and real-world impact," Medium, 2025.A. Zobeiri, M. M. Abootorabi, et al., "Ask in any modality: A comprehensive survey on multimodal retrieval-augmented generation," in Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL), 2025.Y. Hong, H. Zhen, P. Chen, et al., "Advances in 3D generation: A survey," arXiv preprint arXiv:2401.17807, 2024.Z. Li, Y. Wang, et al., "3D scene generation: A survey," arXiv preprint arXiv:2503.02139, 2025.Z. Chen, J. Zhang, et al., "Generative AI meets 3D: A survey on text-to-3D in AIGC era," arXiv preprint arXiv:2402.15208, 2024.Simform, "State of generative AI in 2024: Survey report," Simform Blog, 2024.J.-B. Alayrac, A. Lefèvre, et al., "Interactive holographic display based on finger gestures," Nature, vol. 561, pp. 104–108, 2018.Y. Wang, X. Liu, et al., "Contactless human-computer interaction system based on three-dimensional virtual keyboard," Applied Physics B, vol. 129, no. 5, p. 78, 2023.L. Baraldi, A. Cannavò, et al., "Transforming digital interaction: Integrating immersive holographic communication with metaverse technology," Computers &amp; Education: X Reality, vol. 3, Art. no. 100045, 2025.Hackaday, "Anime inspired holographic virtual assistant," Hackaday Project, 2022."Holographic smart home assistant," International Research Journal of Engineering and Technology (IRJET), vol. 10, no. 6, 2023.A. Mifsud, "Gesture recognition for hologram interaction," ICT Projects, 2021.